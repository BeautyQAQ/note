# Java面试题

## 1.Spring IOC, AOP, DI

### Spring IOC:

**什么是Spring IOC:**

- Spring IOC（控制反转）是一种设计模式和机制，它允许对象的创建和依赖注入控制被外部框架管理。在Spring中，IOC容器负责实例化和配置对象，并为它们提供它们所需依赖项。这有助于解耦组件，使代码更加模块化、易于测试和维护。
- 在Spring IOC中，开发人员不需要手动创建对象或解决它们的依赖关系，而是让Spring容器自动完成这些任务。开发人员只需要告诉Spring容器哪些类应该被实例化，并在需要注入它们所需的依赖项，因此大大简化了代码的编写和维护。
- Spring IOC有两种实现方式：XML配置和注解配置。XML配置是通过一个XML文件来描Bean及其依赖关系，而注解配置则是通过注解将Bean和它们的依赖关系显式地标识出来，加简洁和易于阅读。
- 总的来说，Spring IOC是Spring框架最核心的功能之一，也是实现松耦合、高内聚的要手段，提高了代码的可测试性和可维护性。

**创建 Spring IOC 容器的流程：**
    
- 配置 Spring 的核心配置文件。
- 加载配置文件并解析其中的 Bean 定义。
- 根据 Bean 定义创建并实例化 Bean。
- 将创建好的 Bean 注册到 Bean 容器中。
- 处理 Bean 之间的依赖关系，进行依赖注入。
- 提供 Bean 实例给应用程序使用。

    `在这个过程中，Spring IOC 容器会根据配置文件或者注解信息，自动识别、创建和管理各种 Bean，完成对象的实例化和依赖注入，最终提供这些 Bean 实例给应用程序使用。`

### Spring AOP:

    Spring AOP（面向切面编程）是一种编程范式，允许开发人员将应用程序中的横切关注点进行模块化。它通过提供一种方式来将应用程序的主要业务逻辑与其他行为（如日志记录、安全性或性能监控）分离出来，这些行为被称为切面。 Spring AOP使用代理类拦截方法调用，并在代码中的特定连接点应用方面。这种方法可以帮助开发人员减少代码重复，提高应用程序的可维护性和模块化程度。
    Spring框架提供了基于代理的AOP的实现方式，其核心是使用动态代理技术，在运行时动态地为目标对象创建代理对象，从而在代理对象中添加额外的逻辑。

### Spring DI

依赖注入（Dependency Injection, DI）是一种设计模式，也Spring框架的核心概念之一。其作用是去除Java类之间的依赖关系，实松耦合，以便于开发测试。为了更好地理解DI，先了解DI要解决的问题。[[1](https://zhuanlan.zhihu.com/p/67032669)]。  
在传统的Java开发中，对象之间存在着明显的依赖关系。比如对象A需要用对象B的某些方法或属性，那么对象A就必须自己去创建对象B，并维护象B的生命周期、状态等等。这样做有两个问题：

1. 对象之间的依赖关系非常紧密，一旦某个对象的代码发生变化，其所赖的其他对象的代码也可能需要进行修改。
2. 单元测试困难，无法独立测试某个对象，而必须测试整个依赖链。

通过依赖注入，我们可以将控制权交给容器，由容器去管理对象的创建和毁，同时也负责处理对象的依赖关系。这样做可以让对象之间的关系更散，减少代码的耦合度，提高代码的可维护性和重用性。具体实现方式包构造函数注入、Setter方法注入、接口注入等。Spring框架提供了各种现DI的工具，使得开发人员可以方便地实现依赖注入功能[[2](https:/zhuanlan.zhihu.com/p/67032669)]。

## 2.Spring

### Spring bean的生命周期

1. Spring启动，查找并加载需要被Spring管理的bean，进行Bean的实例化（相当于程序中的new Xx()）
2. Bean实例化后对将Bean的引入和值注入到Bean的属性中
3. 如果Bean实现了BeanNameAware接口的话，Spring将Bean的Id传递给setBeanName()方法(实现BeanNameAware接口主要是为了通过Bean的引用来获得Bean的ID，一般业务中是很少有用到Bean的ID的)
4. 如果Bean实现了BeanFactoryAware接口的话，Spring将调用setBeanFactory()方法，将BeanFactory容器实例传入（（实现BeanFactoryAware 主要目的是为了获取Spring容器，如Bean通过Spring容器发布事件等））
5. 如果Bean实现了ApplicationContextAware接口的话，Spring将调用Bean的setApplicationContext()方法，将bean所在应用上下文引用传入进来。（(作用与BeanFactory类似都是为了获取Spring容器，不同的是Spring容器在调用setApplicationContext方法时会把它自己作为setApplicationContext 的参数传入，而Spring容器在调用setBeanFactory前需要程序员自己指定（注入）setBeanFactory里的参数BeanFactory )
6. 如果Bean实现了BeanPostProcess接口，Spring将调用它们的postProcessBeforeInitialization（预初始化）方法（作用是在Bean实例创建成功后对进行增强处理，如对Bean进行修改，增加某个功能）
7. 如果Bean实现了InitializingBean接口，Spring将调用它们的afterPropertiesSet方法，作用与在配置文件中对Bean使用init-method声明初始化的作用一样，都是在Bean的全部属性设置成功后执行的初始化方法。
8. 如果Bean实现了BeanPostProcess接口，Spring将调用它们的postProcessAfterInitialization（后初始化）方法（作用与6的一样，只不过6是在Bean初始化前执行的，而这个是在Bean初始化后执行的，时机不同 )
9. 此时，Bean已经准备就绪，可以被应用程序使用了。他们将一直驻留在应用上下文中，直到应用上下文被销毁。
10. 如果Bean实现了DisposableBean接口接口，Spring将调用它的destroy方法，作用与在配置文件中对Bean使用destroy-method属性的作用一样，都是在Bean实例销毁前执行的方法。

### Spring bean的初始化流程

1. Bean 的实例化：容器根据配置信息或注解等方式，创建 Bean 的实例。这个过程是通过反射实现的，即利用 Java 反射技术调用无参构造方法来创建 Bean 的实例。
2. 依赖关系处理：如果 Bean 有依赖关系，容器会自动设置这些依赖关系，即通过调用 setter 方法或者直接设置属性的方式将依赖注入到 Bean 中。
3. Bean 的初始化：如果 Bean 实现了 InitializingBean 接口，容器会在实例化、依赖注入完成之后，调用它的 afterPropertiesSet() 方法进行初始化；如果使用 XML 配置文件，则可以通过 <bean> 标签中的 init-method 属性指定 Bean 初始化时需要执行的方法。
4. Bean 的实例化后处理：如果 Bean 实现了 BeanPostProcessor 接口，则在 Bean 实例化后的初始化阶段，容器会调用它的 postProcessBeforeInitialization() 方法和 postProcessAfterInitialization() 方法对 Bean 进行处理。
5. Bean 的销毁：如果 Bean 实现了 DisposableBean 接口，容器在关闭时会调用它的 destroy() 方法进行销毁；如果使用 XML 配置文件，则可以通过 <bean> 标签中的 destroy-method 属性指定 Bean 销毁时需要执行的方法。

    总的来说，Spring Bean 的初始化过程是一个比较复杂的过程，通过掌握这个过程，可以更好地理解 Spring 容器的工作原理，并在开发中避免一些常见的问题。

### spring mvc流程

1. 前端向服务器发送请求，请求被 Spring 前端控制Servlet DispatcherServlet(中央处理器)捕获；
2. DispatcherServlet对请求URL进行解析，得到请求资源标识符（URI）。然后根据该URI，调用HandlerMapping获得该Handler配置的所有相关的对象（包括Handler对象以及Handler对象对应的拦截器），最后以HandlerExecutionChain对象的形式返回给DispatcherServlet(中央处理器)；
3. DispatcherServlet 根据获得的Handler，选择一个合适的HandlerAdapter。（附注：如果成功获得HandlerAdapter后，此时将开始执行拦截器的preHandler(…)方法）
4. 提取Request中的模型数据，填充Handler入参，开始执行Handler（Controller)。 在填充Handler的入参过程中，根据你的配置，Spring将帮你做一些额外的工作：  
HttpMessageConverter： 将请求消息（如Json、xml等数据）转换成一个对象，将对象转换为指定的响应信息数据转换：对请求消息进行数据转换。如String转换成Integer、Double等数据根式化：对请求消息进行数据格式化。 如将字符串转换成格式化数字或格式化日期等数据验证： 验证数据的有效性（长度、格式等），验证结果存储到BindingResult或Error中.
5. Handler执行完成后，向DispatcherServlet 返回一个ModelAndView对象；
6. 根据返回的ModelAndView，选择一个适合的ViewResolver（必须是已经注册到Spring容器中的ViewResolver)返回给DispatcherServlet ；
7. ViewResolver 结合Model和View，来渲染视图
8. 将渲染结果返回给客户端。

### Spring过滤器和拦截器

Spring的拦截器和过滤器都可以用于在请求进入控制器（Controller）之前或之后执行一些预处理或后处理操作。
其中，Spring的拦截器更加精细化，能够针对每个控制器或者控制器中的方法进行配置；而过滤器是基于Servlet API实现的，只能针对URL请求路径进行配置。此外，两者的执行顺序也不同：过滤器优先于拦截器执行，在请求进入Servlet容器时进行预处理；而拦截器是在请求进入Spring MVC的DispatcherServlet后才进行处理。最后，拦截器可以访问处理器上下文（HandlerContext），因此可以获取到更多和业务相关的信息，例如Session、用户信息等；而过滤器则不能。因此，在开发Spring应用时，建议优先选择拦截器来处理相关逻辑。[[1](https://blog.csdn.net/Herishwater/article/details/103544342)][[2](https://zhuanlan.zhihu.com/p/340397290)]

### Spring事物传播特性

    事务的传播，是指一个方法调用另一个方法并将事务传递给它。事务的转播机制主要针对被调用者而言，控制它是否被传播或者被怎样传播。Spring事务的传播机制有七种:

| 传播行为                  | 描述                                                                                                                              |
| :------------------------ | :-------------------------------------------------------------------------------------------------------------------------------- |
| PROPAGATION_REQUIRED      | 默认的Spring事物传播级别，若当前存在事务，则加入该事务，若不存在事务，则新建一个事务                                              |
| PROPAGATION_REQUIRE_NEW   | 若当前没有事务，则新建一个事务。若当前存在事务，则新建一个事务，新老事务相互独立。外部事务抛出异常回滚不会影响内部事务的正常提交 |
| PROPAGATION_NESTED        | 如果当前存在事务，则嵌套在当前事务中执行。如果当前没有事务， 则新建一个事务，类似于REQUIRE_NEW                                    |
| PROPAGATION_SUPPORTS      | 支持当前事务，若当前不存在事务，以非事务的方式执行                                                                                |
| PROPAGATION_NOT_SUPPORTED | 以非事务的方式执行，若当前存在事务，则把当前事务挂起                                                                              |
| PROPAGATION_MANDATORY     | 强制事务执行，若当前不存在事务，则抛出异常                                                                                        |
| PROPAGATION_NEVER         | 以非事务的方式执行，如果当前存在事务，则抛出异常                                                                                  |


Spring 事务在以下情况下可能会失效：

1. 非 Spring 管理的 Service 或 Dao 类中使用了事务注解，这种情况下，由于事务管理器并不知道这些类的存在，所以事务会失效。
2. 异常被 try-catch 捕获并处理后，没有抛出运行时异常或 Error，导致事务无法回滚。
3. 方法中创建了新的线程，新的线程中的事务不会被 Spring 管理。
4. 外部方法调用内部方法，内部方法的事务注解将会失效。
5. 事务的隔离级别设置不当，可能会导致数据不一致。
6. 事务的超时时间过短，可能导致事务执行失败。
7. 事务的传播特性设置不当，可能导致事务无法正常执行。
8. 在事务中使用 Hibernate 的 Session 和 SessionFactory，需要确保它们是与当前线程关联的。
9. 连接池闲置时间过长，导致数据库连接断开，从而事务失效。

以上是 Spring 事务可能失效的一些情况，需要根据实际情况进行分析和解决。

### MyBatis如何拿到spring的事务session

事务控制是Spring管理，sql执行工作是MyBatis在管理，一系列需要事务的sql如何达到事务执行的要求？

需要保证的是Spring拿到的Connection和MyBatis拿到的Connection是同一个即可。

先后的流程我们需要梳理一下，MyBatis在和Spring结合时，我们是配置了一个org.mybatis.spring.SqlSessionFactoryBean的，MyBatis都能通过这个类型拿到SqlSession实例，SqlSession中聚合了Executor，Executor中聚合Transaction，Transaction聚合了Connection。

Connection就是事务的底层控制方式了，而Connection是出自于DataSource，而Spring的org.springframework.jdbc.datasource.DataSourceTransactionManager也注入了DataSource。

所以他们是通过DataSource来联系的，具体的就是操控了从DataSource获取的Connection。

### Spring自定义注解如何实现

Spring 中的自定义注解可以通过 Java 的反射技术来获取 Annotation 对象，在运行时对方法或者类进行动态设置。要自定义注解，需要使用 @interface 关键字声明一个注解，并为其添加相应的属性。示例代码如下：

```java
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
public @interface MyAnnotation {
    String value() default "default";
}
```

以上代码定义了一个自定义注解 MyAnnotation , 该注解具有一个属性 value ，默认值为 "default" 。该注解可以标记在方法上，表示该方法需要被特殊处理。

然后可以在 Spring 的切面中使用该注解，以实现针对被标记方法的特殊处理。示例代码如下：

```java
@Aspect
@Component
public class MyAspect {

    @Around("@annotation(com.example.MyAnnotation)")
    public Object doSomething(ProceedingJoinPoint joinPoint) throws Throwable {
        // 处理逻辑
        return joinPoint.proceed();
    }

}
```

以上代码定义了一个切面 MyAspect ，其中通过 @Around 注解指定了切点表达式，这里使用了 "@annotation(com.example.MyAnnotation)" 以匹配带有 MyAnnotation 注解的方法。在处理逻辑 doSomething 中就可以根据 MyAnnotation 中定义的属性执行特定的操作。

[[2](https://zhuanlan.zhihu.com/p/60730622)][[1](https://spring.io/)]

### Spring Boot

#### **什么是Spring Boot**

Spring Boot是一个基于Spring框架的轻量级开发框架，可以快速构建可独立运行的、生产级别的Java应用。它采用了“约定大于配置”的理念，使得开发者只需很少的配置即可快速搭建一个基于Spring的应用程序。 

Spring Boot提供了起步依赖（Starters），这些依赖管理了类路径中不同的依赖库，简化了项目的构建和维护。同时，Spring Boot还提供了自动化配置（Auto-configuration）功能，通过分析项目的场景和配置，自动完成应用程序所需的各种配置工作。

在开发过程中，Spring Boot提供了一些非常实用的功能，例如内容协商、Web应用、数据访问、安全性和监控等功能，帮助开发者快速构建出高质量的应用程序。

可以通过[[1](https://spring.io/projects/spring-boot/)] 来了解更加详细的 Spring Boot 相关信息，而 [[2](https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started.html)] 则提供了一个入门指南来快速入手 Spring Boot 的开发。

#### **Spring Boot启动流程**

Spring Boot的启动流程大致如下：

1. 通过main函数启动SpringApplication
    - 创建一个空的SpringApplicationContext对象
    - 解析并保存命令行参数
    - 设置应用程序初始上下文（如果指定了）
2. 根据当前环境和应用程序配置创建一个Environment对象
3. 载入所有可用的ApplicationContextInitializer，调用它们的initialize方法，将Environment传递给它们进行初始化 
4. 如果存在SpringApplicationRunListeners，则实例化并调用它们的starting()方法
5. 创建并配置ApplicationContext，包括：
    - 注册所有可用的BeanFactoryPostProcessor实现
    - 注册所有可用的BeanDefinitionRegistryPostProcessor实现
    - 扫描classpath，自动注册所有的bean
    - 注册默认的命名空间处理器
    - 初始化所有单例bean 
6. 如果存在SpringApplicationRunListeners，则实例化并调用它们的environmentPrepared()方法
7. 根据ApplicationContext创建SpringBootBanner
8. 如果存在SpringApplicationRunListeners，则实例化并调用它们的contextPrepared()方法
9. 刷新ApplicationContext，即进行必要的准备工作，例如初始化一些基础设施bean，注册事件监听器等
10. 如果存在SpringApplicationRunListeners，则实例化并调用它们的contextLoaded()方法
11. 启动web容器（如果需要），使应用程序成为一个独立运行的web应用程序 
12. 如果存在SpringApplicationRunListeners，则实例化并调用它们的started()方法
13. 向应用程序中注册所有可用的CommandLineRunner实现
14. 如果存在SpringApplicationRunListeners，则实例化并调用它们的running()方法

以上信息来源于：[[1](https://www.cnblogs.com/Narule/p/14253754.html)] 、[[2](https://zhuanlan.zhihu.com/p/301063931)] 。

#### **Spring Boot启动原理**

Spring Boot 启动的核心原理是基于 Spring 框架，它通过自动配置和约定大于配置的方式实现快速搭建。下面是 Spring Boot 启动的具体流程：

1. 首先，Spring Boot 会读取项目中的配置文件，包括 application.properties，application.yml 等，以获取项目配置信息。
2. 接着，Spring Boot 会扫描项目中的 @SpringBootApplication 注解，并加载该注解所在类及其子包下的所有类。
3. Spring Boot 会根据项目中所引入的依赖来自动配置项目。它会在 classpath 下查找 META-INF/spring.factories 文件，并读取其中的配置项，根据配置项自动进行初始化。
4. 初始化完成后，Spring Boot 会启动内嵌的 Tomcat 或其他服务器，监听 HTTP 请求。
5. 最后，Spring Boot 会将所有已经初始化的 Bean 注册到 Spring 容器中，等待 HTTP 请求的处理。

总的来说，Spring Boot 的启动过程相对比较简单，但自动化的配置使得开发者能够更加专注于业务逻辑的实现。

#### **Spring Boot配置文件加密**

使用jasypt框架

## 3.设计模式

### 使用过那几种,具体的应用

### Spring中使用了哪些设计模式

## 4.MySQL

### MySQL索引

MYSQL索引是一种存储在MySQL数据库中的数据结构，用于加快对数据的检索速度。索引能够提高查询速度，减少查询所需的时间，增加系统的响应速度，并减轻对CPU和磁盘的负载压力。MYSQL索引的设计非常重要，它决定了整体的数据检索性能。

根据[[1](https://zhuanlan.zhihu.com/p/113917726)]，MYSQL索引的作用是实现快速检索，而这是由索引的存储形式和检索引擎的设计来决定的。而[[2](https://www.runoob.com/mysql/mysql-index.html)]则解释了合理使用SQL语句建立索引可以极大的提高MYSQL的查询速度。 合理的MYSQL索引设计和使用，不仅可以提高MYSQL的检索速度，还可以极大的提高系统的响应速度，降低系统出现故障或者宕机的风险。

MySQL索引的原理是基于数据结构和算法实现的。在MySQL中，索引是一种独立的数据结构，它通过对表中某列或多列数据进行排序，存储相应的键值和指向物理存储位置的指针，从而提高数据检索的效率。当查询语句需要访问表中的数据时，MySQL会根据查询条件从索引中查找所需的数据，而不是直接扫描全表，从而极大地提高了检索速度。

MySQL索引的实现原理基于B-Tree和Hash算法，其中B-Tree算法是最常用的索引算法。B-Tree索引采用树形结构来对数据进行排序，并且可以快速定位和访问特定的节点。当数据被加入到B-Tree中时，MySQL会根据其键值将其插入到相应的节点中，从而保证节点中数据的有序性。这样，当需要搜索特定值时，MySQL只需遍历树上最多几个节点即可定位到该值，并返回其指针，从而实现了快速检索。

除了B-Tree索引，MySQL还支持Hash索引、全文索引等多种索引类型。不同的索引类型适用于不同的场景，具体的选择需要根据实际情况进行权衡。

### SQL优化

1. SQL的执行次数分析:  
SQL: show global status like 'com_______'，解释:通过该命令可以查看当前数据库下的增删改查的使用次数,来采取对应的优化处理. value值就是执行的次数。比如替换不同的存储引擎
2. 慢查询日志:
SQL: show variables like '%slow_query_log%'; 慢查询日志会记录超出自己设置的时间还没有执行完毕的sql. 默认情况下，Mysql数据库并不启动慢查询日志，需要我们手动来设置这个参数，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。慢查询日志支持将日志记录写入文件，也支持将日志记录写入数据库表。  
开启慢查询日志SQL: set global slow_query_log=1; 使用set global slow_query_log=1开启了慢查询日志只对当前数据库生效，MySQL重启后则会失效。如果要永久生效，就必须修改配置文件my.cnf  
设置慢查询的超时时间: set global long_query_time=2; 以秒为单位  

3. 使用explain关键字来查看当前SQL语句的执行情况,来对症下药.explain是非常重要的关键字,要善于运用它.通过explain我们可以获得以下信息：
    - 表的读取顺序
    - 数据读取操作的操作类型
    - 哪些索引可以使用
    - 哪些索引被实际使用
    - 表之间的引用
    - 每张表有多少行被优化器查询

4. 正确的建立和使用索引
    - 经常需要搜索的列上，可以加快搜索的速度；
    - 在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。
    - 在经常需要排序的列上创 建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间；
    - 对于中到大型表索引都是非常有效的，但是特大型表的话维护开销会很大，不适合建索引
    - 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度；
    - 避免 where 子句中对字段施加函数，这会造成无法命中索引。
    - 在使用InnoDB时使用与业务无关的自增主键作为主键，即使用逻辑主键，而不要使用业务主键。
    - 删除长期未使用的索引

5. 对SQL语句进行优化
    - SQL语句中IN包含的值不应过多
    - SELECT语句务必指明字段名称
    - 只查询一条数据的时候，使用limit 1
    - 避免在where子句中对字段进行null值判断: 对于null的判断会导致引擎放弃使用索引而进行全表扫描。mysql会自动判断数据的分布情况，判断数据中 null 多还是 not null 多, 然后决定走不走索引. 
    - 避免在where子句中对字段进行表达式操作
    - 对于联合索引来说，要遵守最左前缀法则
    - 尽量使用inner join，避免left join：如果连接方式是inner join，在没有其他过滤条件的情况下MySQL会自动选择小表作为驱动表，但是left join在驱动表的选择上遵循的是左边驱动右边的原则，即left join左边的表名为驱动表（所以如果要使用left join，则要小表驱动大表）。
    - 注意范围查询语句: 对于联合索引来说，如果存在范围查询，比如between、>、<等条件时，会造成后面的索引字段失效。解决办法: 业务允许的情况下,使用 >= 或者<=  这样不影响索引的使用. 当MySQL发现通过索引扫描的行记录数超过全表的10%-30%时，优化器可能会放弃走索引，自动变成全表扫描。
    - 不建议使用%前缀模糊查询:例如 : LIKE“%name”或者LIKE“%name%”，这种查询会导致索引失效而进行全表扫描。但是可以使用LIKE “name%”。
    - 在 where 子句中使用 or 来连接条件，如果or连接的条件有一方没有索引,将导致引擎放弃使用索引而进行全表扫描。解决办法: 将or连接的双方都建立索引,就可以使用. 
    - 应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。(此处存在疑点,测试的时候,发现索引还是能使用到)
    - 字符串类型的字段 查询的时候如果不加引号''  ,会导致自动进行隐式转换,然后索引失效
    - 指定查询的索引，当SQL查询的字段有多个索引的时候,MySQL优化器会自动选择一个索引进行查询,我们也可以通过SQL字段进行自定义

6. 其他优化
    - 需要插入多条数据的时候 使用批量插入.(多次插入需要频繁的建立连接.浪费资源) 
    - 多次插入数据时,采用手动提交事务
    - order by 排序优化 (排序时,使用有索引的字段进行排序) 
    - count 优化，速度:count(*)>count(1)>count(字段)，count( * ):innoDB引擎,特意做了优化,不会取出值,直接服务层进行累加
    - update优化  (避免出现表锁)，innoDB引擎使用update时,会有行锁/表锁两种模式, 如果where 字段没有索引的时候会升级成表锁
    - 创建表时使用统一的编码，mysql多表联查时,如果表的字符集不一样,会有一个数据类型转换的过程. 例如 utf8 与utf8mb4前者是3字节unicode编码，后者是4字节unicode编码. 此时如果多表查询,则索引会失效

常见的 MySQL SQL 优化策略：

1. 使用索引：在 MySQL 中，合理地使用索引是提高 SQL 查询性能的关键。建立适当数量和类型的索引可以提高查询速度，但过多的索引可能会降低性能。因此，在设计数据库时，请根据查询需求建立适当的索引。

2. 避免 SELECT *：尽量减少不必要的列查询，仅查询需要的列，因为 SELECT * 会导致不必要的数据传输和查询时间。

3. 避免在 WHERE 子句中使用函数：使用函数会使索引失效，从而影响查询性能。

4. 避免多个 OR 条件：多个 OR 条件将导致 MySQL 执行全表扫描，影响查询性能。可以通过改写查询语句或者使用 UNION 语句来避免。

5. 使用 INNER JOIN 而不是 WHERE 子句连接表：INNER JOIN 比 WHERE 子句连接表性能更好，尤其是在 join 的表的数据量较大时。

6. 避免使用 ORDER BY 和 DISTINCT：ORDER BY 和 DISTINCT 可以消耗大量的 CPU 资源，尤其是在大数据集上运行时。如果可以，应该尽量避免使用它们。

7. 避免使用子查询：子查询可能会导致 MySQL 执行全表扫描，影响查询性能。应该优先考虑 JOIN 操作。

MySQL SQL 优化不仅需要根据不同的场景进行调整，还需要通过 MySQL 相关工具、分析和监控工具等手段进行实时分析和调整。只有不断地优化和调整 SQL 查询语句，才能让 MySQL 数据库运行地更加高效、稳定和可靠。

### MySQL索引失效的情况

MySQL 索引的失效可能有多种原因，以下是其中的一些常见情况：

1. 参数类型不匹配：当查询条件中的数据类型和索引列的数据类型不一致时，索引无法被使用。例如，在一个 INT 类型的列上建立索引，但是查询条件中传入了一个字符串类型的值。

2. 对索引列做函数操作：如果在索引列上执行函数操作，会导致索引失效，从而强制使用全表扫描。例如，SELECT * FROM table WHERE SUBSTR(name, 1, 1) = 'A'，对于这个查询语句，即使 name 列有索引，MySQL 也无法利用它来加速查询。

3. LIKE 查询的通配符在开头：当使用 LIKE 进行模糊匹配时，如果通配符（%）位于查询字符串开头，则无法使用索引。例如，SELECT * FROM table WHERE name LIKE '%John'，对于这个查询语句，即使 name 列有索引，MySQL 也无法利用它来加速查询。

4. 非连续性索引列查询：当查询语句中包含了非连续性索引列的查询条件时，索引也会失效。例如，对于一个有两个索引列（a、b）的表，查询“WHERE a = 1 AND b = 2”可以使用索引，但查询“WHERE a = 1 AND c = 2”就无法使用索引了。

5. OR查询的优化：当使用 OR 连接多个查询条件时，如果其中一个查询条件无法使用索引，则整个查询都会失效。因此，对于这种情况，可以考虑将 OR 连接的查询条件拆分成多个子查询，以便每个子查询都可以使用索引。

6. 联合索引的问题：在使用联合索引的时候，若某一部分的字段未被包含在查询语句中，则整个联合索引就会失效，因此需要仔细考虑哪些列要建立联合索引。

总之，在实际开发中，要想使用好 MySQL 索引并达到最佳性能，需要仔细设计表结构、合理设置索引，同时运用 EXPLAIN 等工具分析查询计划，及时发现索引失效的原因并及时进行调整。

### MySQL中SQL的执行流程

MySQL 中 SQL 的执行流程包括以下阶段：

1. 连接建立阶段：客户端连接到 MySQL 服务器时，需要进行身份验证。如果身份验证通过，则建立一个连接并分配一个线程给该连接。

2. 语法分析阶段：MySQL 服务器会对客户端提交的 SQL 语句进行语法分析，检查其语法是否正确。如果 SQL 语句有误，则会返回错误信息；否则，服务器会进入下一步处理。

3. 查询优化阶段：在这个阶段中，MySQL 服务器会使用查询优化器，对 SQL 语句进行优化处理，找出最优的执行计划。优化过程包括选择最优的索引、选择最佳的 Join 策略、选择合适的排序算法等。

4. 存储引擎查询阶段：经过优化后的 SQL 语句将会发送到存储引擎进行查询。存储引擎会根据查询语句和索引的情况，找出需要查询的数据，并返回给 MySQL 服务器。

5. 结果集返回阶段：MySQL 服务器会将存储引擎返回的结果集进行整理，然后通过网络返回给客户端。

以上就是 MySQL 中 SQL 的执行流程。在实际应用中，如果要提高 MySQL 的查询性能，可以从优化语句、优化索引、优化服务器硬件资源等多个方面入手，从而提高 MySQL 的查询性能和稳定性。

### MySQL的MVCC

MVCC（Multi-Version Concurrency Control，多版本并发控制）是 MySQL 中一种重要的并发控制技术。

MySQL 内部使用 MVCC 来实现事务隔离级别的可重复读和快照读。MVCC 基于时间戳来实现，在每个数据行上保留了多个版本。

在 MVCC 中，每个事务看到数据库中的一个快照，快照包含了所有已提交事务的“历史”版本。每个事务都可以看到其启动时刻之前提交的所有事务所产生的快照，同时不会看到其他并发事务所做出的改变。

在具体实现上，当一个事务开始执行时，它会被分配一个唯一的 ID（事务 ID），而每个数据行也会有一个版本号。事务中的查询基于读取其启动时刻之前已经存在的行版本，而任何新插入或修改的行都会分配一个新的版本号。

在事务未提交之前，其他事务不可见该事务所做的修改，也就是说，读取应用了 MVCC 的表时，查询只会返回自己启动时期内的版本号满足查询条件的记录，而不受其他事务所影响，从而在保证并发性的同时保证了事务的隔离性。

需要注意的是，MVCC 存在一定的资源开销，因为每个事务所看到的数据版本是不同的，需要额外的空间存储历史版本信息。此外，在高并发的情况下，MVCC 还可能导致锁争用和死锁问题。

因此，在实际使用 MVCC 时，需要对系统负载等因素进行评估，并针对性地优化 MySQL 配置，以保证系统可靠性和性能稳定性。

### MySQL在RR级别下“解决”幻读的方

- 快照读(Snapshot Read / Consistent Read)之间通过MVCC实现。
- 当前读(Current Read / Locking Read)之间由Next-Key Lock(Next-Key-locks是行锁和GAP（间隙锁）的合并)实现。
- 快照读与当前读之间仍然有幻读。

### MySQL的事务隔离级别

1. Read Uncommitted（读未提交）：最低的事务隔离级别，事务中的修改、删除、插入操作都可以被其他并发事务读取到，可能导致脏读、不可重复读和幻读等问题。

2. Read Committed（读已提交）：保证一个事务提交后才能被另一个并发事务读取，解决了脏读的问题，但可能会出现不可重复读和幻读的问题。

3. Repeatable Read（可重复读）：保证在同一个事务中多次读取同一数据时返回的结果是一致的，解决了不可重复读的问题，但可能会出现幻读的问题。

4. Serializable（串行化）：最高的事务隔离级别，该级别下所有的事务串行执行，避免了脏读、不可重复读和幻读等问题，但会导致数据库性能降低。

- 脏读：指一个事务读取到了另一个事务未提交的“脏”数据。例如，在一个事务中更新了一行数据，但尚未提交，另一个并发事务读取到了这个未提交的数据，这就是脏读。

- 不可重复读：指在同一事务中，读取同一数据两次得到的结果不一致。例如，一个事务在读取某一行数据后，另一个并发事务对该行数据进行修改或删除，导致第一个事务再次读取同一行数据时，得到与前一次不同的结果。

- 幻读：指在同一事务中，执行相同的查询操作，但返回的结果集合不一样。例如，在一个事务中查询某一范围内的所有数据行，但在该事务在执行期间另一个并发事务插入了一些满足该范围的新数据行，导致第一个事务再次查询时，得到了更新后的结果集合。

    需要注意的是，脏读、不可重复读和幻读三种问题的产生都是由于并发事务之间的互相干扰所导致的。为了避免这些问题，可以使用事务隔离机制来限制不同事务之间的并发访问。

### 乐观锁和悲观锁

**悲观锁:** 假定数据并发冲突,屏蔽一切可能违法数据完整性的操作,在查询数据完就把事务锁起来,知道提交事务

**乐观锁:** 假定不会发生并发冲突,只在提交时检查是否违反数据完整性,在修改数据的时候吧事务锁起来,通过version来锁定,实现方式:一般使用版本号或者cas算法来实现

## 5.分布式

### 分布式的CAP理论

CAP理论是指分布式系统中三个核心概念之间的平衡关系：一致性（Consistency）、可用性（Availability）和分区容错性（Partition Tolerance）。CAP理论认为，在分布式系统中，无法同时满足这三个目标，只能在它们之间进行权衡或者选择其中的两个来保证分布式系统的正常运行。

一致性是指分布式系统中的数据始终保持同步和正确性。在一个满足强一致性要求的系统中，任何时刻任何节点读取到的数据都是一致的。可用性是指系统必须在几乎所有的请求中都能够返回合适的响应结果。在一个满足高可用性要求的系统中，系统的响应速度和可用性非常高，但是这可能会导致数据的不一致性。分区容错性是指分布式系统中的某些节点可能由于网络故障、硬件问题等原因丢失连接，但是整个系统仍然能够继续运作。

根据CAP理论，分布式系统只能同时满足其中的两个要求，并且必须牺牲另一个目标。例如，在保证一致性和分区容错性的前提下，可用性可能会受到一定程度的影响。在实际应用中，需要根据系统的需求和实际情况来选择其中的两个目标权衡。

### 分布式锁

    使用redis的分布式锁。具体做法是后端接收到请求后加入一个分布式锁，如果加锁成功，就执行业务，如果加锁失败就等待锁或者拒绝请求。业务执行完成后释放锁。

使用的是SETNX命令：`SETNX KEY_NAME VALUE`。设置成功返回1，设置失败返回0

1. 如果加锁成功的客户端挂了怎么办？
   可以设置一个过期时间，命令：`SET key value [EX seconds] [PX milliseconds] NX`
2. 设置了过期时间，如果业务还没有执行完成，但是redis锁过期了，怎么办？
   需要对锁进行续约。设置锁成功后，启动一个watchdog，每隔一段时间(比如10s)为当前分布式锁续约，也就是每隔10s重新设置当前key的超时时间。`EXPIRE <key> <seconds>`
3. watchdog怎么实现呢？
   当客户端加锁成功后，可以启动一个定时任务，每隔10s(最好支持配置)来检测业务是否处理完成，检测的依据就是判断分布式锁的key是否还存在，如果存在，就进行续约。
4. 如果当前线程已经处理完，这个key是被其他客户端写入的呢？
   可以为每个客户端指定一个clientID，在VALUE中增加一个clientID的前缀，这样在续锁的时候，可以判断当前分布式锁的value前缀来确定是不是当前客户端的，如果是再续锁，否则不做处理。
5. 如果client1宕机了，这时分布式锁还可以续期吗？
   因为分布式锁的续期是在客户端执行的，所以如果client1宕机了，续期线程就不能工作了，也就不能续期了。这时应该把分布式锁删除，让其他客户端来获取。
6. 那如果client1宕机了，其他客户端需要等待30s才能有机会获取到锁，有办法立刻删除锁吗？
   因为client1宕机了，只能等到超时时间后锁被自动删除。如果要立刻删除，需要增加额外的工作，比如增加哨兵机制，让哨兵来维护所有redis客户端的列表。哨兵定时监控客户端是否宕机，如果检测到宕机，立刻删除这个客户端的锁。这里的哨兵并不是redis的哨兵，而且为了检测客户端故障业务系统自己做的哨兵。

### RPC 请求，如何保证幂等性

对于 RPC 请求，为了保证幂等性，可以采用以下几种方式：

1. 请求端生成唯一标识符

在每次发送 RPC 请求时，请求端可以生成唯一标识符，并将该标识符随着请求一起发送给服务端。服务端在接收到请求后，首先判断该标识符是否已经处理过该请求，如果是，则直接返回之前的响应结果即可。否则，服务端需要处理该请求，并将处理结果和请求标识符缓存起来。这样，即使客户端在网络传输中出现重试或消息丢失，服务端都能够正确地处理请求，并返回相同的结果。

2. 利用数据库事务实现幂等性

当 RPC 请求需要修改数据库数据时，可以通过利用数据库事务实现幂等性。通常情况下，并发的数据库访问可能会导致幂等性问题。但是，数据库事务机制能够在事务过程中锁定相关数据，从而防止并发修改数据的情况发生。因此，当一个 RPC 请求需要修改数据库数据时，可以将其包裹在一个数据库事务中，从而实现幂等性。

3. 全局编号与去重

为了防止请求重复执行，通常采用全局编号与去重的方法。RPC 请求可以在全局范围内生成唯一的编号，同时将该编号作为请求的一部分发送给服务端。服务端在处理请求之前，会先检查该编号是否已经存在于去重集合中，如果是，则说明该请求已经被处理，直接返回之前的响应结果即可。否则，服务端需要将该编号添加到去重集合中，并处理请求，最后返回处理结果。

这些方法都可以很好地保证 RPC 请求的幂等性，但是需要根据具体的业务场景和技术实现情况来选择。

### RPC 请求，如何保证幂等性

在 RPC 请求中，如何保证一致性，以下是几种常见的做法：

1. 采用分布式事务

分布式事务是通过在跨多个节点和服务之间协调执行的一组事务操作来实现的。在 RPC 请求中，如果需要修改多个服务中的数据，可以使用分布式事务来保证数据的一致性。分布式事务通常需要使用一些分布式事务管理器来实现，例如 TCC、Saga 等。

2. 异步处理

如果 RPC 请求需要处理大量数据或者需要执行耗时的操作，可以使用异步处理机制来保证系统的一致性。这种机制通常是将 RPC 请求发送到消息队列中进行异步处理，然后发送响应结果。在整个过程中，异步任务可以在后台完成，而不会影响主线程的执行效率。

3. 主从复制

在一些场景下，RPC 请求的数据处理都是针对一个主服务节点进行的，其他从节点需要保持与主节点的数据同步。可以使用主从复制机制来实现数据的同步。当主节点处理数据后，从节点可以通过拉取数据或者订阅更新来保持数据的一致性。

综上所述，RPC 请求的一致性可以通过分布式事务、异步处理和主从复制等方法来实现。具体选用哪种方法，需要根据具体的业务需求、数据存储方式和技术实现情况等方面进行综合考虑。

## 6.JVM

### jvm参数调优

1. jvm调优涉及到两个很重要的概念：吞吐量和响应时间。jvm调优主要是针对他们进行调整优化，达到一个理想的目标，根据业务确定目标是吞吐量优先还是响应时间优先。  
吞吐量：用户代码执行时间/(用户代码执行时间+GC执行时间)。  
响应时间：整个接口的响应时间(用户代码执行时间+GC执行时间)，stw时间越短，响应时间越短。  

    调优的前提是熟悉业务场景，先判断出当前业务场景是吞吐量优先还是响应时间优先。调优需要建立在监控之上，由压力测来判断是否达到业务要求和性能要求。  调优的步骤大致可以分为：
   - 熟悉业务场景，了解当前业务系统的要求，是吞吐量优先还是响应时间优先；
   - 选择合适的垃圾回收器组合，如果是吞吐量优先，则选择ps+po组合；如果是响应时间优先，在1.8以后选择G1，在1.8之前选择ParNew+CMS组合；
   - 规划内存需求，只能进行大致的规划。
   - CPU选择，在预算之内性能越高越好；
   - 根据实际情况设置升级年龄，最大年龄为15；
   - 设定日志参数：-Xloggc:/path/name-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogs=5 -XX:GCLogFileSize=20M -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCCauses
 
```txt
-XX:+UseGCLogFileRotation：GC文件循环使用
-XX:NumberOfGCLogs=5：使用5个GC文件
-XX:GCLogFileSize=20M：每个GC文件的大小
上面这三个参数放在一起代表的含义是：5个GC文件循环使用，每个GC文件20M，总共使用100M存储日志文件，当5个GC文件都使用完毕以后，覆盖第一个GC日志文件，生成新的GC文件。
```

2. 对JVM内存的系统级的调优主要的目的是减少GC的频率和Full GC的次数。导致Full GC的原因一般是年老代（Tenured）被写满，调优时尽量让对象在新生代GC时被回收、让对象在新生代多存活一段时间和不要创建过大的对象及数组避免直接在年老代创建对象。

    - 1.监控GC的状态
    - 2.生成堆的dump文件，通过JMX的MBean生成当前的Heap信息，大小为一个3G（整个堆的大小）的hprof文件，如果没有启动JMX可以通过Java的jmap命令来生成该文件。
    - 3.分析dump文件，Visual VM
    - 分析结果，判断是否需要优化，如果各项参数设置合理，系统没有超时日志出现，GC频率不高，GC耗时不高，那么没有必要进行GC优化，如果GC时间超过1-3秒，或者频繁GC，则必须优化。

### 垃圾收集器

JDK8中默认的是UseParallelGC 即 Parallel Scavenge + Parallel Old

[参考资料](https://blog.csdn.net/xushiyu1996818/article/details/102761595)

#### **Serial 收集器**

这个收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅是说明它只会使用一个CPU或一条收集线程去完成垃圾收 集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程（这件事情称之为“Stop The World”），直到它收集结束.**新生代采用标记-复制算法，老年代采用标记-整理算法。**

特点是简单而高效（与其他收集器的单线程比），对 于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收 集自然可以获得最高的单线程收集效率。

#### **ParNew 收集器**

ParNew收集器其实就是Serial收集器的多线程版本，除了使用多条线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数。**新生代采用标记-复制算法，老年代采用标记-整理算法。**

其中有一个与功能、性能无关但其实很重要的原因是:除了Serial收器外,目前只有它能与CMS收集器配合工作。自JDK 9开始，可以理解为ParNew合并入CMS,成为它专门处理新生代的组成部分。ParNew可以说是HotSpot虚拟机中第一款退出历史舞台的垃圾收集器。

#### **Parallel Scavenge 收集器**

Parallel Scavenge收集器也是一个新生代收集器，它也是使用复制算法的收集器， 又是并行的多线程收集器（**新生代采用标记-复制算法，老年代采用标记-整理算法。**）, Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目 标则是达到一个可控制的吞吐量(Throughput)。  

停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户的体验；而高吞吐量则可以最高效率地利用CPU时间，尽快地完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。  

由于与吞吐量关系密切，Parallel Scavenge收集器也经常被称为“吞吐量 优先”收集器。除上述两个参数之外，Parallel Scavenge收集器还有一个参 数-XX:+UseAdaptiveSizePolicy值得关注。这是一个开关参数，当这个参数打开之后，就 不需要手工指定新生代的大小（・Xmn）、Eden与Survivor区的比例（・XX:SurvivorRatio）、 晋升老年代对象年龄（・XX:PretenureSizeThreshold）等细节参数了，虚拟机会根据当 前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或 最大的吞吐量，这种调节方式称为GC自适应的调节策略（GC Ergonomics）

#### **CMS收集器**

CMS (Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器，是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。从名字(包含“MarkSweep”)上就可以看出CMS收集器是基于“标记-清除”算 法实现的.

CMS运作过程:
- 初始标记(CMS initial mark)
- 并发标记(CMS concurrent mark)
- 并发预清理
- 重新标记(CMS remark)
- 并发清除(CMS concurrent sweep)

其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。

CMS是一款优秀的收集器，它的最主要优点在名字上已经体现出来了：并发收集、 低停顿，Sun的一些官方文档里面也称之为并发低停顿收集器(Concurrent Low Pause Collector)。但是CMS还远达不到完美的程度，它有以下四个显著的缺点：

- 对 CPU 资源敏感
- 无法处理浮动垃圾，可能会导致serial old
- 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生
- 需要预留的空间，给用户线程使用

#### **G1收集器**

G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征.

作为CMS收集器的替代者和继承人,设计者们希望做出一款能够建立起“停顿时间模型” (PausePrediction Model)的收集器,停顿时间模型的意思是能够支持指定在一个长度为M毫秒的时间片段内,消耗在垃圾收集上的时间大概率不超过N毫秒这样的目标,这几乎已经是实时Java (RTSJ)的中软实时垃圾收集器特征了。  
在G1收集器出现之前的所有其他收集器,包括CMS在内,垃圾收集的目标范围要么是整个新生代(Minor GC) ,要么就是整个老年代(Major GC) ,再要么就是整个Java堆(Full GC)。而G1跳出了这个樊笼,它可以面向堆内存任何部分来组成回收集(Collection Set,一般简称CSet)进行回收,衡量标准不再是它属于哪个分代,而是哪块内存中存放的垃圾数量最多,回收收益最大,这就是G1收集器的Mixed GC模式。

G1 收集器采用一种不同的方式来管理堆内存.堆内存被划分为多个大小相等的 heap 区,每个heap区都是逻辑上连续的一段内存(virtual memory). 其中一部分区域被当成收集器相同的角色(eden, survivor, old), 但每个角色的区域个数都不是固定的。这在内存使用上提供了更多的灵活性。

### **ZGC收集器**

ZGC（The Z Garbage Collector）是JDK 11中推出的一款低延迟垃圾回收器，它的设计目标包括：
- 停顿时间不超过10ms；
- 停顿时间不会随着堆的大小，或者活跃对象的大小而增加；
- 支持8MB~4TB级别的堆（未来支持16TB）。
- 从设计目标来看，我们知道ZGC适用于大内存低延迟服务的内存管理和回收。
- 与CMS中的ParNew和G1类似，ZGC也采用标记-复制算法，不过ZGC对该算法做了重大改进：ZGC在标记、转移和重定位阶段几乎都是并发的，这是ZGC实现停顿时间小于10ms目标的最关键原因。

## 7.netty

### netty的线程模型

## 8.流量网关

### sentinel

**sentinel核心**

- 1.资源(服务,接口,方法,链路)
- 2.规则(限流的判断规则),总的并发数,并发的线程数量

流量控制后的处理方法

- 1.抛异常
- 2.排队
- 3.降级

限流规则由下面几个因素组成

- resource:资源名,即限流规则的对象  
- count:限流阀值  
- grade:限流阀值的裂类型(QPS,或者并发线程数)  
- limitAPP:流控的针对的调用来源 若为default则不区分来源  
- strategy:限流规则  
- controlBehavior:流量控制的效果(直接拒绝,warm up(冷启动),匀速排队)

**QPS流量控制**  

1. 直接拒接

    直接拒绝（RuleConstant.CONTROL_BEHAVIOR_DEFAULT）方式是默认的流量控制方式，当 QPS超过任意规则的阈值后，新的请求就会被立即拒绝，拒绝方式为抛出FlowException。 这种方式适用于对系统处理能力确切已知的情况下，比如通过压测确定了系统的准确水位时。

2. Warm Up

    Warm Up（RuleConstant.CONTROL_BEHAVIOR_WARM_UP）方式，即预热/冷启动方式。当系 统长期处于低水位的情况下，当流量突然增加时，直接把系统拉升到高水位可能瞬间把系统压 垮。通过"冷启动"，让通过的流量缓慢增加，在一定时间内逐渐增加到阈值上限，给冷系统一个预 热的时间，避免冷系统被压垮。

    以下都会随着系统访问量增加逐步预热来提升性能的因素.  

    - 缓存预热 
    - 数据库连接池初始化

    `可以设置预热的时间`

3. 匀速排队

    匀速排队（RuleConstant.CONTROL_BEHAVIOR_RATE_LIMITER）方式会严格控制请求通过 的间隔时间，也即是让请求以均匀的速度通过，其实对应的是漏桶算法。 当请求数量远远大于阈值时，这些请求会排队等待，这个等待时间可以设置，如果超过等待时间，那这个请求会被拒绝。

4. 熔断降级

    当访问量比较高的请求下，一个后端依赖节点的延迟响应可能导致所有服务器上的所有资源在 数秒内饱和。一旦出现这个问题，会导致系统资源被快速消耗，从而导致服务宕机等问题，最坏的情况会导致服务雪崩

    所以在软件系统中，为了防止这种问题的产生，也引入了熔断的概念。所以，熔断的意义是： 如果某个目标服务调用比较慢或者大量的超时，这个时候如果触发熔断机制，则可以保证后续 的请求不会继续发送到目标服务上，而是直接返回降级的逻辑并且快速释放资源。如果目标服务的情况恢复了，那么熔断机制又会动态进行关闭。

如何判定资源不稳定呢？

1. 慢调用比例（SLOW_REQUEST_RATIO）。 
2. 异常比例（ERROR_RATIO）。 
3. 异常数（ERROR_COUNT）

**动态流控**  

我们可以通过Sentinel Dashboard或者Config CenterDashboard把流控规则推送到统一的配置中心（Nacos、Zookeeper等），客户端通过实现 ReadableDataSource接口来监听配置中心，从而实时获取变更的规则实现动态控制。

**DataSource 扩展常见的实现方式有:**

拉模式：客户端主动向某个规则管理中心定期轮询拉取规则，这个规则中心可以是 RDBMS、文件，甚至是 VCS 等。这样做的方式是简单，缺点是无法及时获取变更； 

推模式：规则中心统一推送，客户端通过注册监听器的方式时刻监听变化，比如使用 Nacos、Zookeeper 等配置中心。这种方式有更好的实时性和一致性保证。

## 9.Java基础

### java四大特性

### 静态变量和普通变量的区别

### 说说工作中用到的设计模式，我背了个代理模式

### HashMap的扩容机制

HashMap 是一种常用的哈希表数据结构，它的内部实现采用了数组 + 链表 / 红黑树的方式来存储键值对。当哈希表中的元素数量达到一定阈值时，HashMap 会触发扩容操作，以便更好地维护哈希表的性能和识别度。

具体来说，HashMap 的扩容机制主要包括以下几个步骤：

1. 当 HashMap 中的元素数量达到阈值(capacity * loadFactor) 时，就需要进行扩容。默认的负载因子(loadFactor)为0.75，也可以通过构造函数自定义。

2. 定义一个新的数组（容量是原数组的两倍），将原数组中的所有节点重新分配到新数组中。这个过程需要调用 putVal() 方法重新计算每个元素在新数组中的位置，并将其存储在新数组中。

3. 扩容完成后，原数组中的元素不再被使用，会被 GC 回收。新数组成为了当前的数组。

需要注意的是，由于 rehash 操作需要重新计算每个元素在新数组中的位置，因此扩容操作是比较耗时的。因此，为了提高哈希表的性能，需要合理设置哈希表的容量和负载因子，以减少扩容操作的次数。

## 10.多线程

### 什么是线程安全问题

线程安全问题是指多个线程同时访问共享资源时可能会出现的数据不一致或者异常情况。举例来说，当多个线程同时对同一个变量进行写操作时，很容易出现数据覆盖的情况，从而导致程序出现错误。

下面以一个具体的例子来说明线程安全问题：

```java
public class ThreadTest {
    private static int count = 0;

    public static void main(String[] args) {
        for (int i = 0; i < 10; i++) {
            new Thread(() -> {
                // 对count进行100次自增操作
                for (int j = 0; j < 100; j++) {
                    count++;
                }
            }).start();
        }

        try {
            Thread.sleep(1000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

        System.out.println("count=" + count);
    }
}
```

上述代码中，我们启动了10个线程，每个线程对`count`进行100次自增操作。但是由于`count++`并不是一个原子操作，它实际上包含了3个步骤：读取`count`的值、将`count`的值+1、将新的值保存到`count`中。如果多个线程同时执行`count++`，就有可能出现数据竞争的情况。

运行上述代码，可能会得到如下的结果（每次运行结果可能不同）：

```
count=764
```

可以看到，预期的结果应该是`count=1000`，但实际上却只是764。这是因为多个线程同时对`count`进行写操作，导致了数据不一致的问题。

针对上述问题，可以使用`synchronized`关键字对`count++`操作进行加锁，以保证只有一个线程可以执行该操作：

```java
public class ThreadTest {
    private static int count = 0;

    public static void main(String[] args) {
        for (int i = 0; i < 10; i++) {
            new Thread(() -> {
                // 对count进行100次自增操作
                for (int j = 0; j < 100; j++) {
                    synchronized (ThreadTest.class) { // 加锁
                        count++;
                    }
                }
            }).start();
        }

        try {
            Thread.sleep(1000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

        System.out.println("count=" + count);
    }
}
```

这样，我们就可以保证多个线程访问`count`时不会出现竞争和数据不一致的问题。

### HashMap在哪部分会出现线程不安全问题

`HashMap`在进行并发访问时，主要存在以下两个线程不安全的问题：

1. 线程间竞争导致的数据不一致：由于`HashMap`是通过哈希函数将键映射到存储桶中，多个键可能会被映射到同一个存储桶中。当多个线程同时对同一个存储桶进行插入、删除等操作时，就有可能导致该存储桶的链表结构被破坏，从而导致数据不一致。

2. 扩容导致的死循环(JDK1.7)：在`HashMap`内部，当元素数量达到一个阈值时，会触发扩容操作。扩容时需要将原有的元素重新分配到新的存储桶中，但如果有多个线程同时在并发地对`HashMap`进行修改，就有可能会导致在赋值过程中出现覆盖的情况，从而导致扩容后的`HashMap`中存在数据丢失。此外，在扩容过程中，如果多个线程同时对某个存储桶进行读写操作，就有可能出现死循环的情况，从而导致程序挂起。
    
    为什么JDK1.7并发扩容会导致死循环

    HashMap在进行扩容操作时，需要将原有的元素重新分配到新的存储桶中。为了保证分配后的新存储桶中的链表结构不被破坏，HashMap在进行分配时采用了一种叫做“头插法”的方式，具体来说，就是将元素插入到链表头部。

    而问题在于，当多个线程同时对同一个存储桶进行插入操作时，就有可能出现链表节点覆盖的情况，导致链表结构发生改变，从而可能导致死循环。

    举个例子，假设此时HashMap中有两个存储桶A和B，他们在旧表中的位置相邻，且都要分别在新表中对应的位置上创建链表。同时，线程1正在对存储桶A进行元素的重分配并添加，线程2正在对存储桶B进行元素的重分配并添加，由于线程1比线程2先一步执行，所以线程1会先将存储在A中的元素插入到新表中的对应位置。但是，当线程2也将B中的元素插入到对应位置时，它会将之前由线程1插入的元素覆盖掉，从而导致该链表上的元素漏掉一部分，进而触发死循环。

为了解决这些线程不安全的问题，可以使用`ConcurrentHashMap`，它是一个线程安全的哈希表，采用了锁分段技术来实现线程安全，并且扩容过程不会发生死循环。或者使用`synchronized`来对`HashMap`进行加锁操作，以保证多个线程不会同时对同一个存储桶进行操作。但这样会显著降低`HashMap`的并发性能。

### 获取线程返回结果

可以通过使用Java的Future对象来获取线程执行的结果。在多线程执行任务时，可以将任务封装成实现了Callable接口的类，并用FutureTask类来包装该类的对象。FutureTask类实现了Runnable接口，因此它可以作为一个线程来执行。在主线程中，可以先调用FutureTask的get()方法来获取任务执行的结果。如果任务尚未完成，主线程会阻塞等待。当任务完成后，主线程就可以拿到任务的返回值。

参考知识来源链接：[[1](https://www.cnblogs.com/coder-programming/articles/11955844.html)]

或者使用`CompletableFuture`:

`CompletableFuture` 是 Java 8 引入的异步编程工具，它基于 `Future` 和 `Promise` 设计实现。与传统的 `Future` 相比，`CompletableFuture` 提供了更好的异步编程体验。在 Java 9 中，`CompletableFuture` 进一步增强，并提供了更多的方法支持，使得 Java 的异步编程变得更加简单和便捷。

`CompletableFuture` 可以被显式地完成(通过设置其值和状态)，并且可以用作 CompletionStage，支持依赖函数和在其完成时触发的操作。当两个或多个线程尝试完成、completeExceptionally 或取消 CompletableFuture 时，只有一个线程成功。除了这些直接操纵状态的方法之外，还有一些其他方法可用于组合和转换 CompletableFuture。

Java 9 为 `CompletableFuture` 提供了一些新特性，如超时和延迟支持、更好的继承支持和实用工具方法等。在代码方面，API 增加了八个新方法和五个新的静态方法。

参考知识来源链接：[[1](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html)][[2](https://www.baeldung.com/java-9-completablefuture)]


`CompletableFuture` 的使用方法如下：

1. 创建 `CompletableFuture` 对象。可以使用 `CompletableFuture.supplyAsync()` 或者 `CompletableFuture.runAsync()` 方法创建一个异步任务，这些方法会返回一个已经启动了的 `CompletableFuture` 对象。

2. 添加回调函数。通过 `thenApply()`, `thenAccept()`, `thenRun()`, `thenCompose()`, 或 `thenCombine()` 等方法，将原始 `CompletableFuture` 对象与一个回调函数关联起来，使得在 `CompletableFuture` 对象执行完毕后，将会自动执行回调函数。

3. 组合多个 `CompletableFuture` 对象。通过 `allOf()` 或者 `anyOf()` 方法，可以将多个 `CompletableFuture` 对象组合起来进行并发操作，从而加速程序的执行。

4. 能够让主线程等待 `CompletableFuture` 完成。`CompletableFuture` 对象的 `join()` 方法能够让主线程等待当前 `CompletableFuture` 对象执行完成，从而避免了使用 `get()` 方法时可能产生的异常抛出。

下面是一个简单的 `CompletableFuture` 的例子：

```
CompletableFuture.supplyAsync(() -> "Hello World")
    .thenApply(s -> s + " - CompletableFuture")
    .thenAccept(System.out::println)
    .join();
```
在这个例子中，我们创建了一个新的 `CompletableFuture` 对象，并通过 `thenApply()` 方法将其与一个回调函数关联起来。回调函数处理字符串拼接操作，之后通过 `thenAccept()` 方法将回调函数的结果打印出来。最后通过 `join()` 方法等待 `CompletableFuture` 对象执行完毕。

参考知识来源链接：[[1](https://www.baeldung.com/java-completablefuture)]

### 线程生命周期

线程的生命周期包括五个阶段：

1.新建状态(New): 当一个线程对象被创建后，它处于新建状态。此时它已经拥有了代码运行所需的所有资源，但还没有开始执行。

2.就绪状态(Ready): 线程一旦进入就绪状态，就表示它已经准备好了运行，只要CPU有时间，它随时可以开始执行。

3.运行状态(Running): 线程进入运行状态后，CPU开始执行线程的代码。

4.阻塞状态(Blocked): 当线程需要等待某个条件满足时，比如等待IO操作完成，它会进入阻塞状态。此时它不会占用CPU时间片，直到等待的条件满足后才会进入就绪状态。

5.终止状态(Terminated): 当线程的run()方法执行完毕时，它就进入终止状态。线程一旦进入这种状态，就表示它已经完成了自己的使命，不再需要运行。[[1](https://baike.baidu.com/item/%E7%BA%BF%E7%A8%8B/103101)]

### 高并发下该使用什么集合

在高并发的情况下，Java中应该使用线程安全的集合，以避免多个线程同时读写从而导致数据不一致的问题。以下是几个常用的线程安全集合：

1. ConcurrentHashMap

    `ConcurrentHashMap`是一个线程安全的哈希表，比`Hashtable`和`synchronizedMap`的性能更好。它支持高并发的读写操作，并且在多线程情况下具有更好的可伸缩性。因此，如果需要进行高并发的读写操作，可以优先考虑使用`ConcurrentHashMap`。

2. CopyOnWriteArrayList

    `CopyOnWriteArrayList`是一个线程安全的动态数组，它的读操作不需要锁定，因此非常适合读多写少的场景。在写操作时，会复制出一份新的数组来进行操作，因此写操作的性能较差。如果需要进行读多写少的操作，可以使用`CopyOnWriteArrayList`。

3. ConcurrentLinkedQueue

    `ConcurrentLinkedQueue`是一个非阻塞的队列，也是一个线程安全的队列。它采用单向链表的形式来存储数据，因此在多线程情况下具有更好的性能，并且可以保证读取和写入的顺序一致。如果需要进行高并发的队列操作，可以考虑使用`ConcurrentLinkedQueue`。

4. BlockingQueue

    `BlockingQueue`可以看作是一个阻塞的队列，它提供了`put()`和`take()`方法来进行读写操作。如果队列已满，调用`put()`方法的线程会被阻塞，直到有元素被移除；如果队列为空，调用`take()`方法的线程会被阻塞，直到有元素被插入。如果需要进行生产-消费模型的操作，可以考虑使用`BlockingQueue`。

需要注意的是，在使用线程安全的集合时，虽然可以保证线程安全，但也可能出现死锁、性能下降等问题。因此，在使用这些集合时，还需要根据具体的业务场景进行合理的选择和使用。

### 线程池执行流程

当提交一个新任务，线程池的处理流程如下：

1. 判断线程池中核心线程数是否已达阈值corePoolSize,若否，则创建一个新核心线程执行任务
2. 若核心线程数已达阈值corePoolSize,判断阻塞队列workQueue是否已满，若未满，则将新任务添加进阻塞队列
3. 若满，再判断，线程池中线程数是否达到阈值maximumPoolSize,若否，则新建一个非核心线程执行任务。若达到阈值，则执行**线程池饱和策略**。

线程池饱和策略分为一下几种：

- AbortPolicy:直接抛出一个异常，默认策略
- DiscardPolicy: 直接丢弃任务
- DiscardOldestPolicy:抛弃下一个将要被执行的任务(最旧任务)
- CallerRunsPolicy:主线程中执行任务

几种典型的工作队列:

- ArrayBlockingQueue:使用数组实现的有界阻塞队列，特性先进先出
- LinkedBlockingQueue:使用链表实现的阻塞队列，特性先进先出，可以设置其容量，默认为Interger.MAX_VALUE，特性先进先出
- PriorityBlockingQueue:使用平衡二叉树堆，实现的具有优先级的无界阻塞队列
- DelayQueue:无界阻塞延迟队列，队列中每个元素均有过期时间，当从队列获取元素时，只有过期元素才会出队列。队列头元素是最块要过期的元素。
- SynchronousQueue:一个不存储元素的阻塞队列，每个插入操作，必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态

## 11.算法

### 红黑树和B+树的优劣

红黑树和B+树是两种非常常用的数据结构，它们都有着很好的性能表现，但是在不同场景下会有不同的优缺点，下面是它们的一些对比：

红黑树：
- 优点：红黑树的查询、插入、删除操作都是O(logn)级别的，这样可以保证其在大规模的数据处理中有着较好的性能表现。
- 缺点：查找区间数据时效率较低。

B+树：
- 优点：B+树（非叶子节点不保存具体的数据，而只保存关键字的索引）索引节点只存储键值信息，可以容纳更多的键值信息，因此可以减少磁盘I/O次数，提高查询效率。因为所有数据必须要到叶子节点才能获取到，所以每次数据查询的次数都一样，这样一来B+树的查询速度也就会比较稳定。同时，在B+树中，叶节点上存储的数据也可以形成链表，支持范围查询。
- 缺点：B+树（多路平衡树）的查询、插入、删除操作也是O(logn)级别的，但比起红黑树有略微的劣势，同时由于B+树的节点比较大，因此在查询单条数据时可能会带来额外的开销。

综上所述，如果对于查询大块数据的场景，B+树会更加优秀；如果对于查询单条数据和频繁更新的场景，红黑树会更好一些。但是这并不是绝对的，具体应该根据实际情况选择合适的数据结构。

## 12.队列和缓存

### Redis 缓存过期删除机制

Redis 无论有没有设置expire， 都会遵循redis的配置好的删除机制， 在配置文件里设置：
```txt
redis 最大内存不足时,数据清除策略,默认为: volatile-lru
    
volatile-lru  ->  
    "过期集合"中的数据采取LRU(近期最少使用)算法。 
    如果对key使用"expire"指令指定了过期时间, 那么此key将会被添加到"过期集合"中。 
    首先将已经过期的数据优先移除. 若不能满足最大内存maxMemory设置, 
    继续使用LRU算法删除设置expire但还是没有过期的key,
    如果"过期集合" 中全部移除仍不能满足内存需求,将OOM.
 
allkeys-lru ->   对所有的数据,采用LRU算法,
 
volatile-random ->  对"过期集合"中的数据采取"随即选取"算法,并移除选中的K-V,直到"内存足够"为止. 
                    如果"过期集合" 中全部移除全部移除仍不能满足,将OOM
 
allkeys-random ->  对所有的数据,采取"随机选取"算法,并移除选中的K-V,直到"内存足够"为止
 
volatile-ttl ->     对"过期集合"中的数据采取TTL算法(最小存活时间),移除即将过期的数据.
        
no-enviction ->     不做任何干扰操作,直接返回OOM异常
```

- 定时删除: 在设置键过期时间的同时,创建一个定时器,让定时器在过期时间来临时,立即执行对键的删除操作
- 惰性删除: 放任键过期不管,但是每次从键空间获取键时,都检查取得的键是否过期,过期的话就删除该键,没过期的话就返回该键
- 定期删除: 每隔一段时间,程序就对数据库进行一次检查,删除里面的过期键,至于要删除多少个过期键,检查多少个数据库,由算法决定

**Redis所用策略**：惰性删除和定期删除相结合,但是Redis定期删除是随机抽取机制,不可能扫描删除掉所有的过期Key,因此需要内存淘汰机制.

### redis怎么做mq

Redis可以通过**发布订阅模式**实现消息队列（MQ）的功能。下面是使用Redis作为消息队列的一个简单示例：

1. 生产者向队列中添加消息
   
```java
import redis.clients.jedis.Jedis;

public class RedisProducer {
    public static void main(String[] args) throws InterruptedException {
        // 连接redis
        Jedis jedis = new Jedis("localhost", 6379);

        // 生产者往名为“myqueue”的队列中添加消息
        jedis.lpush("myqueue", "hello");

        // 关闭连接
        jedis.close();
    }
}
```

2. 消费者监听队列并处理消息

```java
import redis.clients.jedis.Jedis;

public class RedisConsumer {
    public static void main(String[] args) {
        // 连接redis
        Jedis jedis = new Jedis("localhost", 6379);

        // 定义消费者函数
        while (true) {
            // 使用brpop命令从名为“myqueue”的队列中取出消息
            String message = jedis.brpop(0, "myqueue").get(1);

            // 处理消息，这里只是简单地打印消息内容
            System.out.println(message);
        }

        // 关闭连接
        jedis.close();
    }
}
```

上述代码中，生产者通过Redis中的lpush命令将消息添加到myqueue队列中，消费者则通过Redis中的brpop命令实现了阻塞式的队列监听，当有消息进入队列时，就可以立即进行消费处理。

需要注意的是，Redis虽然可以作为轻量级的MQ使用，但是在实际生产环境中，还需要考虑诸如持久化、消息重发等问题，这时就需要使用专业的MQ系统，比如Kafka、RabbitMQ等。

### RabbitMQ如何绑定队列和交换机

RabbitMQ中，要将队列和交换机绑定在一起，需要使用`queueBind`命令，该命令的语法为：

```java
channel.queueBind(String queue, String exchange, String routingKey)
```

其中，`queue`表示要绑定的队列名称，`exchange`表示要绑定的交换机名称，`routingKey`表示路由键，用于指定交换机将消息发送给哪些队列。

以Java代码为例，下面是一个简单的示例：

```java
import com.rabbitmq.client.Channel;
import com.rabbitmq.client.Connection;
import com.rabbitmq.client.ConnectionFactory;

public class RabbitMqBinder {
    private static final String QUEUE_NAME = "myQueue";
    private static final String EXCHANGE_NAME = "myExchange";
    private static final String ROUTING_KEY = "myKey";

    public static void main(String[] argv) throws Exception {
        ConnectionFactory factory = new ConnectionFactory();
        factory.setHost("localhost");
        try (Connection connection = factory.newConnection();
             Channel channel = connection.createChannel()) {
            // 声明队列
            channel.queueDeclare(QUEUE_NAME, false, false, false, null);
            // 声明交换机
            channel.exchangeDeclare(EXCHANGE_NAME, "direct");
            // 将队列和交换机绑定在一起
            channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, ROUTING_KEY);

            System.out.println("绑定成功！");
        }
    }
}
```

上述代码中，我们声明了一个名为`myQueue`的队列和一个名为`myExchange`、类型为`direct`的交换机。然后，通过`queueBind`命令将队列和交换机绑定在一起，路由键为`myKey`。这样，当其他生产者通过`myExchange`向RabbitMQ发送消息时，该消息就会被路由到绑定了`myKey`的`myQueue`队列中。

需要注意的是，在实际使用中，还需要考虑到交换机类型、交换机和队列的持久化等问题，以保证消息的可靠性和稳定性。

### Kafka的零拷贝

Kafka 的零拷贝技术是指在数据传输时避免不必要的数据拷贝，从而减少内存复制和 CPU 消耗，提高了数据传输的效率。在 Kafka 中，零拷贝技术是通过采用操作系统的 mmap 和 sendfile 系统调用实现的。

具体来说，Kafka 利用了操作系统提供的缓存机制，将文件中的数据映射到缓存空间中，然后使用 sendfile 系统调用直接将数据从缓存中发送出去，避免了数据在内核和用户空间之间的复制，从而减少了 CPU 的消耗。

此外，在生产者端，Kafka 还使用了内存池技术来重复利用已经分配的缓冲区，避免频繁的内存分配和释放，从而提高了数据传输的效率。

总的来说，Kafka 利用了操作系统提供的高效 IO 系统调用和内存管理技术，实现了对数据的零拷贝，能够大大降低数据传输时的 CPU 开销，提高数据传输的效率和性能。在大规模数据传输场景中，零拷贝技术的应用可以显著提高数据传输的效率，减少系统资源的消耗，进而提升整个系统的性能和可靠性。

### Kafka 和 RabbitMQ的区别和各自的优劣

Kafka 和 RabbitMQ 都是流行的消息队列中间件，它们的设计和应用场景有着一些区别。下面是它们的主要区别和各自的优劣：

1. 设计目标不同：Kafka 的设计目标是高吞吐量、低延迟、高可靠性和可扩展性，适用于大规模数据处理场景；RabbitMQ 的设计目标是简单易用，提供了诸如消息持久化、消息确认、消息顺序等多种可信赖的特性，适用于较小规模的分布式系统。

2. 通信协议不同：Kafka 使用原生 TCP 协议进行通信，传输效率更高；RabbitMQ 使用 AMQP (Advanced Message Queuing Protocol) 协议，支持多种编程语言，更加灵活；此外，RabbitMQ 还支持其他的通信协议，如 STOMP、MQTT 等。

3. 消息路由方式不同：Kafka 采用发布-订阅模型，消费者按照主题订阅消息，消息被广播到所有的订阅者中；RabbitMQ 支持多种路由方式，比如 Direct、Topic、Fanout 等，消费者只能接收与其绑定的交换机中的消息。

4. 重复消费和消息顺序保证不同：由于 Kafka 只能保证消息越过多个分区的顺序，但无法保证整个主题内的消息顺序和不重复消费；RabbitMQ 提供了严格的消息确认和事务机制，能够保证消息送达和处理的可靠性，同时保证消息的顺序性和不重复消费。

5. 存储方式不同：Kafka 采用基于磁盘的持久化存储，数据可以长期保留，并通过批量压缩和索引技术来提高读写效率；RabbitMQ 支持多种后端存储，如基于内存、基于磁盘等，在各自场景下灵活运用。

综上所述，Kafka 和 RabbitMQ 都有其优缺点，需要根据具体的业务需求和场景选择使用。如果是大规模数据处理场景，需要高吞吐量、低延迟和高可靠性，那么 Kafka 更为适合；如果是较小规模的分布式系统，对消息的可信赖性要求更高，那么 RabbitMQ 更为适合。

### Kafka如何保证消息的跨分区有序性

Kafka分布式的单位是partition，同一个partition用一个write ahead log组织，所以可以保证FIFO的顺序。不同partition之间不能保证顺序。

### 如何保证数据库和缓存的一致性

- 先删除缓存，再更新数据库需要用到延迟双删策略，并发场景下的延迟双删策略，这个延迟时间很难评估，所以推荐【先更新数据库，再删除缓存】的方案，这个方案一般来说更新数据库会比查询更慢，所以这个删除操作理论上会在查询线程的后面进行，会大大降低出现一致性问题的概率。
- 在【先更新数据库，再删除缓存】的方案下，为了保证两步都能执行成功，需要配合【消息队列】或【订阅变更日志】的方案来做，其本质是通过重试的方式保证数据一致性。
- 在【先更新数据库，再删除缓存】方案下，【读写分离 + 主从延迟】也会导致缓存和数据库不一致，解决问题的方案是【延迟双删】，凭借经验发送【延迟消息】到队列中，延迟删除缓存，同时要也要控制主从库延迟（可以通过暂时剔除延迟高的节点，延迟低的时候再将节点加入集群），尽可能降低不一致发生的概率。
- 如果要保证强一致性，可以将mysql的update 和delete redis的操作放到一个分布式锁中，redis get 操作如果获取到的数据为null，则使用分布式锁锁住然后查询mysql获取最新的值，确保更新，删除，查询三个操作在同一个锁中完成，简单的代码大概这样：
```
redis.lock()
delete redis;
update mysql;
redis.unlock;

object o=redis.get();
if(o==null){
    while(!redis.isLock()){
        select mysql;
        redis.set();
    }
}
```

## 13.其他

### Reactive Programming(响应式编程)有什么优势

Java 的 Reactive Programming 是一种编程范式，它主要基于响应式流（Reactive Streams）标准，并借鉴了函数式编程和事件驱动编程的思想。相比传统的命令式编程方式，Reactive Programming 具有以下优势：

1. 异步非阻塞：Reactive Programming 采用异步回调方式处理数据流，使代码在执行时不会阻塞线程，从而提高了运行效率。

2. 响应式编程：Reactive Programming 将数据流看作事件流，通过对事件进行响应式编程实现数据的处理和转换，简化了代码的逻辑结构。

3. 函数式编程：Reactive Programming 借鉴了函数式编程的思想，以数据流为基本单位，通过组合和变换等函数式操作实现对数据的处理，使代码更加清晰、简洁。

4. 可扩展性：Reactive Programming 设计上注重可扩展性，能够很好地满足大规模分布式系统的需求，支持水平扩展和负载均衡等高级特性。

5. 与现有框架兼容：Java 的 Reactive Programming 支持与现有框架的无缝集成，例如 Spring Framework，能够很好地与传统的 Web 应用开发模式相结合使用。

综上所述，Java 的 Reactive Programming 在开发大规模、高并发、分布式应用等方面拥有很多优势，能够提高代码的可读性、可维护性、可扩展性和性能表现。

Reactive 模式效率提升的核心原因是它采用了异步非阻塞的方式处理数据流，相比传统同步阻塞式模式，它具有以下优势：

1. 更好地利用系统资源：在 Reactive 模式中，每个线程可以同时处理多个请求的响应，不会被阻塞，从而提高系统资源利用率。

2. 减少上下文切换开销：传统同步阻塞式模式中，当线程执行 I/O 操作时会被阻塞，需要等待 I/O 操作完成才能继续执行，这样会频繁地进行线程上下文的切换，带来较大的开销；而在 Reactive 模式中，通过使用回调函数实现异步非阻塞的 I/O 操作，避免了线程的阻塞，从而减少了上下文切换的开销。

3. 支持更高的并发量：在 Reactive 模式下，由于每个线程可以同时处理多个请求的响应，因此系统的并发量可以得到有效提升，从而更好地支持高并发场景需求。

4. 更低的延迟：在 Reactive 模式中，通过异步非阻塞的方式处理数据流，任务的执行时间得到有效压缩，从而避免了任务堆积导致的较高延迟。

综上所述，Reactive 模式相比传统同步阻塞式模式具有更好的系统资源利用率、更少的上下文切换开销、更高的并发量和更低的延迟等优势，因此能够有效提高系统的处理效率。

### http接口的内容

### 层网络模型

应用层->表示层->会话层->传输层->网络层->数据链路层->物理层

### sharding-jdbc多主多从配置 主从如何进行数据同步

### 为什么选择sharding-jdbc进行分库分表的一个设计 

当前的技术选型有二种,一种是mycat,一个是sharding-jdbc,mycat 基于proxy内存实现mysql协议,可以当做数据库来使用,sharding-jdbc基于jdbc的一个增强,性能高

### 线上定位问题调试

通过日志，做好日志打印，方便各种问题调试

### 死锁、如何避免死锁

死锁是指二个或多个事务在同一资源上相互占用,并请求锁定对方的资源,从而导致恶性循环  

**解决死锁的方法:**

- 1.如果不同程序并发存取多张表,尽量以约定相同的顺序访问表,可大大降低死锁
- 2.在同一个事务中,尽可能的一次锁全部的资源,减少锁产生的概率
- 3.对于非常容易产生死锁的业务部分,可能尝试使用升级锁定的粒度,通过表锁来减少锁产生的概率
- 4.如果业务不好处理,可以采用分布式事务锁,或者乐观锁

### 如何找出热点缓存数据

在业务表中增加一个字段，查询一次标记一次，次数到达一定值之后作为热点数据，在业务逻辑中处理。

### 使用的什么流水线部署, Jenkins, GitHub Actions

### 项目遇到的问题

- 1.要配置熔断器的超时时间，否则默认时间是1s，很容易报错
- 2.用来传输数据的result类要有一个默认的构造函数，否则会有一个类型定义错误

## 13.场景题

### 30w数据，每个数据30个标签，需要筛选，怎么设计表

### 500w带索引的数据, 如何快速查询

分批查询, 创建10个线程， 每个线程查询50w，使用CountDownLatch等待线程执行完毕

### 给一个Jar包，如何取消Jar包中的认证

如果你的 Jar 包中包含了认证机制，可以通过以下几种方式来取消认证：

1. 将认证代码进行注释或删除

如果你有源代码可用，可以简单地注释掉或删除认证代码。这种方法非常简单，但需要对代码做出修改。

2. 使用反射机制绕过认证

如果你不能对源代码进行修改，你可以使用 Java 的反射机制来绕过认证。可以将认证相关的代码放到一个单独的类中，并在应用程序启动时通过反射调用该类，从而执行认证前的逻辑，然后跳过认证过程。这种方法不需要对源代码进行修改，但是需要对该类进行反射调用，所以需要一定的编程能力。

3. 使用类加载器绕过认证

如果你无法对源代码进行修改，并且反射机制无法绕过认证，你可以使用 Java 的类加载器来绕过认证。使用自定义的类加载器，在加载 Jar 包时，可以使用自定义的类加载器替代系统类加载器，从而在加载时修改认证相关的类或方法，达到绕过认证的目的。这种方法需要对类加载器和字节码操作有较深的了解。

需要注意的是，取消认证可能会导致 Jar 包的安全性下降，请谨慎使用。如果你需要取消认证，请确保只在测试环境中使用，不要在生产环境中使用。

### 大对象变小如何存入redis

Kafka和rabbit分别实现多消费者

项目难点

多线程参数，超时时间

延时任务用哪个线程池

spring cloud Alibaba套件

open feign

负载均衡

面试题笔试题回答上来的有大概以下:谈一谈OOP，详细说一下数据类型，谈一谈继承、多态，谈一谈接口和抽象类的区别，三目运算符，
说一下六个设计原则，说几个会的设计模式及应用场景，手写单例模式（线程安全），手写冒泡排序、快速排序，聊一聊集合框架（问的很多，涉及底层实现），
手写实现文件的复制，介绍一下多线程（主要是不同的创建方式及其相关概念），Mysql相关概念及其优化（索引问得多），Servlet生命周期，jsp的9个内置对象，
filter的配置及应用，Spring的IOC和DI，AOP，Struts2、Spring、SpringMVC的工作流程及其配置文件，Spring及Spring boot的相关注解，
MyBatis和Hibernate的不同点及各自优缺点。以上是知识点，临场表达能力就靠自己了。        
以下是没有回答上来的：谈一谈JVM，笛卡儿积，红黑树的实现，nio，Linux搜索空文件夹并在每个文件夹中创建制定文件，
Sql语句也有很多写不出来，前端框架会多少，

喜欢那些新技术



<!-- 面试历程: 
腾讯外包 -- 寄
尤尼信息 -- 过
众鼎（平安外包） -- 过
机大呷 -- 寄
鑫泉网络 -- 寄
租客网 -- 寄
云台一号 -- 寄
南山医疗公司（基本没问啥） -- 寄
南方电网的笔试题 -- 寄
金贝塔 -- 过
赣州银行外包面 -- 过
宽拓外包 -- 等待中 -->